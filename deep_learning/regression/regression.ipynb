{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eDfFpD_be58",
    "outputId": "f37aba0b-01f5-405f-c4ec-7d3e09b8d5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_tuner in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras_tuner) (3.4.1)\n",
      "Requirement already satisfied: packaging in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras_tuner) (24.1)\n",
      "Requirement already satisfied: requests in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras_tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from keras->keras_tuner) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from requests->keras_tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from requests->keras_tuner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from requests->keras_tuner) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from requests->keras_tuner) (2024.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from optree->keras->keras_tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from rich->keras->keras_tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from rich->keras->keras_tuner) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\dev\\udemy\\python\\py-env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "keUpycTsbpEr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbNwaOy4bv6S",
    "outputId": "58a37153-b073-4f8b-a4cd-4cd1a94fbb3b"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive')\n",
    "else:\n",
    "    path = r'C:/dev/machine_learning_project/deep_learning/classification/'\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mytnuiWb1Iz"
   },
   "source": [
    "#Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "957wRJsQb5Au"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('../../data/chiefs_knife_dataset.xlsx')\n",
    "index_Ra = dataframe.columns.get_loc('Ra') # index of the surface roughness column for inserting the class. label\n",
    "\n",
    "lower_specification_limit = 0.125 # lower bound of good quality product region\n",
    "upper_specification_limit = 0.215  # upper bound of good quality product region\n",
    "\n",
    "is_between_specification_bounds = (dataframe['Ra']>=lower_specification_limit) & (dataframe['Ra'] < upper_specification_limit)\n",
    "good_product_range = np.where(is_between_specification_bounds,\"good product\",\"poor product\")\n",
    "dataframe.insert(index_Ra +1, 'Quality',good_product_range) # encoding: good quality product := 1 and poor quality product := 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlkbDJHCcYPm"
   },
   "source": [
    "# constructing Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wDrXRplMcelq"
   },
   "outputs": [],
   "source": [
    "X = dataframe.loc[:,'Original_Linienanzahl':'DFT_Median_sobel_Bereich'].values\n",
    "y = dataframe['Ra'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RetU2TIdAPC"
   },
   "source": [
    "#Splitting dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OPqpuDhpd8Hd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ghYbDmL_dKV9"
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmzvTBgQeid-"
   },
   "source": [
    "#Scaling the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cknpgDskePMn"
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gwh1FU_e4kk"
   },
   "source": [
    "#Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mym4Y5eqfAU6"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LeakyReLU, ELU, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Nadam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "class DeepRegressionModel:\n",
    "  def __init__(self):\n",
    "      \"\"\"\n",
    "      initialize the hyperparameters for hyperparameter tuning\n",
    "      \"\"\"\n",
    "      self.neurons_max = 1024\n",
    "      self.neurons_min = 8\n",
    "      self.neurons_step = 16\n",
    "      self.learning_rates = [1e-2, 1e-3, 1e-4,1e-5]\n",
    "      self.activation_name = ['relu', 'leaky_relu', 'elu', 'selu']\n",
    "      self.optimizer_name = ['adam', 'nadam','rmsprop','sgd']\n",
    "\n",
    "  def build_model(self,hp):\n",
    "      \"\"\"\n",
    "      create a NN architecture for the classifiction task\n",
    "      hp: hyperparameter for hyperparameter tuning\n",
    "      \"\"\"\n",
    "      model = Sequential()\n",
    "\n",
    "      model.add(Dense(units = hp.Int('hidden_1', min_value=self.neurons_min, max_value=self.neurons_max, step=self.neurons_step),\n",
    "                      activation =self.set_activation(hp.Choice('activation_1', values=self.activation_name)),input_dim=X_train.shape[1]))\n",
    "\n",
    "      model.add(Dense(units = hp.Int('hidden_2', min_value=self.neurons_min, max_value=self.neurons_max, step=self.neurons_step),\n",
    "                      activation = self.set_activation(hp.Choice('activation_2', values=self.activation_name))))\n",
    "\n",
    "      model.add(Dense(units = hp.Int('hidden_3', min_value=self.neurons_min, max_value=self.neurons_max, step=self.neurons_step),\n",
    "                      activation = self.set_activation(hp.Choice('activation_3', values=self.activation_name))))\n",
    "\n",
    "      model.add(Dense(units = hp.Int('hidden_4', min_value=self.neurons_min, max_value=self.neurons_max, step=self.neurons_step),\n",
    "                      activation = self.set_activation(hp.Choice('activation_4', values=self.activation_name))))\n",
    "\n",
    "      model.add(Dense(units = hp.Int('hidden_5', min_value=self.neurons_min, max_value=self.neurons_max, step=self.neurons_step),\n",
    "                      activation = self.set_activation(hp.Choice('activation_5', values=self.activation_name))))\n",
    "\n",
    "      model.add(Dense(units= 1, activation =self.set_activation(hp.Choice('activation_out', values=self.activation_name))))\n",
    "\n",
    "      model.compile(optimizer =self.set_optimizer(hp.Choice('optimizer', values=self.optimizer_name), hp.Choice('learning_rate', values=self.learning_rates)) ,\n",
    "                    loss='mean_squared_error',metrics=[tf.keras.metrics.R2Score(name='r_squared')])\n",
    "      return model\n",
    "\n",
    "  def set_activation(self, activation_name):\n",
    "      \"\"\"\n",
    "      set the activation function for the model\n",
    "      \"\"\"\n",
    "      if activation_name == 'leaky_relu':\n",
    "        activation = LeakyReLU()\n",
    "      elif activation_name == 'elu':\n",
    "        activation = ELU()\n",
    "      elif activation_name == 'selu':\n",
    "        activation = Activation('selu')\n",
    "      else:\n",
    "        activation = Activation(activation_name)\n",
    "      return activation\n",
    "\n",
    "  def set_optimizer(self, optimizer_name, learn_rate):\n",
    "      \"\"\"\n",
    "      set the optimizer for the model\n",
    "      \"\"\"\n",
    "      if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learn_rate)\n",
    "      elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learn_rate)\n",
    "      elif optimizer_name == 'nadam':\n",
    "        optimizer = Nadam(learning_rate=learn_rate)\n",
    "      elif optimizer_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learn_rate)\n",
    "      return optimizer\n",
    "\n",
    "  # def r_squared(self,y_true, y_pred):\n",
    "  #     \"\"\"\n",
    "  #     calculate R^2 metric for model evaluation\n",
    "  #     y_true: true label\n",
    "  #     y_pred: predicted label\n",
    "  #     \"\"\"\n",
    "  #     y_mean = tf.reduce_mean(y_true)\n",
    "  #     residual = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "  #     total_sum_of_squares  = tf.reduce_sum(tf.square(y_true - y_mean))\n",
    "  #     r2 = 1 - residual / (total_sum_of_squares)\n",
    "  #     # r2 = 1 - residual / (total_sum_of_squares + tf.keras.backend.epsilon())\n",
    "  #     return r2\n",
    "\n",
    "  # def adjusted_r_squared(self,y_true, y_pred):\n",
    "  #     \"\"\"\n",
    "  #     calculate adjusted R^2 metric for model evaluation\n",
    "  #     y_true: true label\n",
    "  #     y_pred: predicted label\n",
    "  #     \"\"\"\n",
    "  #     n = tf.cast(tf.shape(X_train)[0], tf.float32)\n",
    "  #     p = tf.cast(tf.shape(X_train)[1], tf.float32)\n",
    "\n",
    "  #     r2 = self.r_squared(y_true, y_pred)\n",
    "  #     adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "  #     return adjusted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC8iT9uG97-C"
   },
   "source": [
    "#Random search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dM42UKA8-4X",
    "outputId": "32df8553-1742-4090-bf47-8c63573ae582"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Udemy\\Python\\py-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "regressor = DeepRegressionModel()\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "hypermodel=regressor.build_model,\n",
    "objective=kt.Objective(\"val_r_squared\", direction=\"max\"),\n",
    "max_trials=50,\n",
    "executions_per_trial=1,\n",
    "overwrite=True,\n",
    "directory=\"hyperparameter_tuning_regressor\",\n",
    "project_name=\"knife_regressor\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "bNbUuPs-A_X8",
    "outputId": "8db44408-d4b9-4015-8d8c-160646a149ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 99 Complete [00h 02m 16s]\n",
      "val_r_squared: 0.6067884564399719\n",
      "\n",
      "Best val_r_squared So Far: 0.6597249507904053\n",
      "Total elapsed time: 03h 57m 53s\n",
      "\n",
      "Search: Running Trial #100\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "424               |280               |hidden_1\n",
      "leaky_relu        |elu               |activation_1\n",
      "376               |648               |hidden_2\n",
      "selu              |elu               |activation_2\n",
      "232               |1000              |hidden_3\n",
      "relu              |relu              |activation_3\n",
      "936               |8                 |hidden_4\n",
      "elu               |selu              |activation_4\n",
      "136               |616               |hidden_5\n",
      "leaky_relu        |leaky_relu        |activation_5\n",
      "selu              |leaky_relu        |activation_out\n",
      "rmsprop           |rmsprop           |optimizer\n",
      "0.001             |0.001             |learning_rate\n",
      "\n",
      "Epoch 1/15\n",
      "\u001b[1m766/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.2072 - r_squared: -52.1266 - val_loss: 0.0021 - val_r_squared: 0.4607\n",
      "Epoch 2/15\n",
      "\u001b[1m557/766\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0029 - r_squared: 0.3627"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "tuner.search(X_train, y_train, batch_size=8, epochs=15, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KjZW01FMBx13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hyperparameter_tuning\\simple\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_r_squared\", direction=\"max\")\n",
      "\n",
      "Trial 095 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 280\n",
      "activation_1: elu\n",
      "hidden_2: 648\n",
      "activation_2: elu\n",
      "hidden_3: 1000\n",
      "activation_3: relu\n",
      "hidden_4: 8\n",
      "activation_4: selu\n",
      "hidden_5: 616\n",
      "activation_5: leaky_relu\n",
      "activation_out: leaky_relu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.6597249507904053\n",
      "\n",
      "Trial 039 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 872\n",
      "activation_1: selu\n",
      "hidden_2: 40\n",
      "activation_2: selu\n",
      "hidden_3: 184\n",
      "activation_3: leaky_relu\n",
      "hidden_4: 88\n",
      "activation_4: leaky_relu\n",
      "hidden_5: 680\n",
      "activation_5: elu\n",
      "activation_out: elu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.0001\n",
      "Score: 0.6542302370071411\n",
      "\n",
      "Trial 047 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 280\n",
      "activation_1: leaky_relu\n",
      "hidden_2: 376\n",
      "activation_2: relu\n",
      "hidden_3: 696\n",
      "activation_3: elu\n",
      "hidden_4: 8\n",
      "activation_4: leaky_relu\n",
      "hidden_5: 744\n",
      "activation_5: elu\n",
      "activation_out: elu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.6477838158607483\n",
      "\n",
      "Trial 091 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 776\n",
      "activation_1: relu\n",
      "hidden_2: 168\n",
      "activation_2: leaky_relu\n",
      "hidden_3: 328\n",
      "activation_3: relu\n",
      "hidden_4: 472\n",
      "activation_4: selu\n",
      "hidden_5: 616\n",
      "activation_5: elu\n",
      "activation_out: elu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.6458093523979187\n",
      "\n",
      "Trial 099 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 424\n",
      "activation_1: leaky_relu\n",
      "hidden_2: 376\n",
      "activation_2: selu\n",
      "hidden_3: 232\n",
      "activation_3: relu\n",
      "hidden_4: 936\n",
      "activation_4: elu\n",
      "hidden_5: 136\n",
      "activation_5: leaky_relu\n",
      "activation_out: selu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.6452119946479797\n",
      "\n",
      "Trial 025 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 296\n",
      "activation_1: relu\n",
      "hidden_2: 568\n",
      "activation_2: leaky_relu\n",
      "hidden_3: 456\n",
      "activation_3: elu\n",
      "hidden_4: 136\n",
      "activation_4: relu\n",
      "hidden_5: 536\n",
      "activation_5: elu\n",
      "activation_out: elu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.0001\n",
      "Score: 0.6364412307739258\n",
      "\n",
      "Trial 021 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 504\n",
      "activation_1: leaky_relu\n",
      "hidden_2: 408\n",
      "activation_2: leaky_relu\n",
      "hidden_3: 616\n",
      "activation_3: leaky_relu\n",
      "hidden_4: 216\n",
      "activation_4: relu\n",
      "hidden_5: 216\n",
      "activation_5: relu\n",
      "activation_out: elu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.6359382271766663\n",
      "\n",
      "Trial 069 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 520\n",
      "activation_1: selu\n",
      "hidden_2: 40\n",
      "activation_2: relu\n",
      "hidden_3: 232\n",
      "activation_3: relu\n",
      "hidden_4: 456\n",
      "activation_4: leaky_relu\n",
      "hidden_5: 616\n",
      "activation_5: leaky_relu\n",
      "activation_out: selu\n",
      "optimizer: adam\n",
      "learning_rate: 0.0001\n",
      "Score: 0.6356408596038818\n",
      "\n",
      "Trial 030 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 456\n",
      "activation_1: selu\n",
      "hidden_2: 504\n",
      "activation_2: leaky_relu\n",
      "hidden_3: 616\n",
      "activation_3: selu\n",
      "hidden_4: 152\n",
      "activation_4: relu\n",
      "hidden_5: 408\n",
      "activation_5: leaky_relu\n",
      "activation_out: selu\n",
      "optimizer: rmsprop\n",
      "learning_rate: 0.001\n",
      "Score: 0.629939079284668\n",
      "\n",
      "Trial 019 summary\n",
      "Hyperparameters:\n",
      "hidden_1: 264\n",
      "activation_1: leaky_relu\n",
      "hidden_2: 488\n",
      "activation_2: leaky_relu\n",
      "hidden_3: 504\n",
      "activation_3: leaky_relu\n",
      "hidden_4: 24\n",
      "activation_4: leaky_relu\n",
      "hidden_5: 664\n",
      "activation_5: relu\n",
      "activation_out: selu\n",
      "optimizer: adam\n",
      "learning_rate: 1e-05\n",
      "Score: 0.6295740604400635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\Udemy\\Python\\py-env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "models = tuner.get_best_models(num_models=8)\n",
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py-env",
   "language": "python",
   "name": "py-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
